reticulate::repl_python()
quit
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(reticulate)
reticulate::repl_python()
import requests
from bs4 import BeautifulSoup
import pandas as pd
url = 'https://en.wikipedia.org/wiki/Category:Italian_Renaissance_sculptors'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
data = []
table = soup.find('div', {'id': 'mw-pages'})
links = table.find_all('a')
for link in links:
page_1_name = link.text
page_1_url = link['href']
if page_1_url.startswith('/wiki/') and not page_1_name.startswith('This list may not'):
data.append({'page_1_name': page_1_name, 'page_1_url': page_1_url})
df_pages = pd.DataFrame(data)
df_pages['page_id'] = df_pages.index + 1
crosslinks = pd.DataFrame(index = df_pages['page_id'], columns = df_pages['page_id']).fillna(0)
for _, row in df_pages.iloc[1:].iterrows():
page_1_id = row['page_id']
page_1_url = row['page_1_url']
page_1_soup = BeautifulSoup(
requests.get('https://ru.wikipedia.org' + page_1_url).content,
'html.parser')
links = page_1_soup.find_all('a')
for link in links:
if link.has_attr('href') and link['href'].startswith('/wiki/'):
page_2_url = link['href']
page_2_name = link.text
if page_2_url in df_pages['page_1_url'].values and page_2_url != page_1_url:
page_2_id = df_pages.loc[df_pages['page_1_url'] == page_2_url, 'page_id'].iloc[0]
crosslinks.loc[page_1_id, page_2_id] = 1
View(page_1_name)
df_pages = df_pages[['page_id', 'page_1_name']]
print(df_pages)
print(crosslinks.iloc[:, -10:])
for _, row in df_pages.iloc[1:].iterrows():
page_1_id = row['page_id']
page_1_url = row['page_1_url']
page_1_soup = BeautifulSoup(
requests.get('https://ru.wikipedia.org' + page_1_url).content,
'html.parser')
links = page_1_soup.find_all('a')
for link in links:
if link.has_attr('href') and link['href'].startswith('/wiki/'):
page_2_url = link['href']
page_2_name = link.text
if page_2_url in df_pages['page_1_url'].values and page_2_url != page_1_url:
page_2_id = df_pages.loc[df_pages['page_1_url'] == page_2_url, 'page_id'].iloc[0]
crosslinks.loc[page_1_id, page_2_id] = 1
print(crosslinks.iloc[:, -10:])
for _, row in df_pages.iloc[1:].iterrows():
page_1_id = row['page_id']
page_1_url = row['page_1_url']
page_1_soup = BeautifulSoup(
requests.get('https://ru.wikipedia.org' + page_1_url).content,
'html.parser')
links = page_1_soup.find_all('a')
for link in links:
if link.has_attr('href') and link['href'].startswith('/wiki/'):
page_2_url = link['href']
page_2_name = link.text
print(f"page_1_url: {page_1_url}")
print(f"page_2_url: {page_2_url}")
if page_2_url in df_pages['page_1_url'].values and page_2_url != page_1_url:
page_2_id = df_pages.loc[df_pages['page_1_url'] == page_2_url, 'page_id'].iloc[0]
print(f"page_2_id: {page_2_id}")
crosslinks.loc[page_1_id, page_2_id] = 1
for _, row in df_pages.iloc[1:].iterrows():
page_1_id = row['page_id']
page_1_url = row['page_1_url']
page_1_soup = BeautifulSoup(
requests.get('https://ru.wikipedia.org' + page_1_url).content,
'html.parser')
links = page_1_soup.find_all('a')
for link in links:
if link.has_attr('href') and link['href'].startswith('/wiki/'):
page_2_url = link['href']
page_2_name = link.text
print(f"page_1_url: {page_1_url}")
print(f"page_2_url: {page_2_url}")
if page_2_url in df_pages['page_1_url'].values and page_2_url != page_1_url:
page_2_id = df_pages.loc[df_pages['page_1_url'] == page_2_url, 'page_id'].iloc[0]
print(f"page_2_id: {page_2_id}")
crosslinks.loc[page_1_id, page_2_id] = 1
quit
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(reticulate)
reticulate::repl_python()
import requests
from bs4 import BeautifulSoup
import pandas as pd
url = 'https://en.wikipedia.org/wiki/Category:Italian_Renaissance_sculptors'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
data = []
table = soup.find('div', {'id': 'mw-pages'})
links = table.find_all('a')
for link in links:
page_1_name = link.text
page_1_url = link['href']
if page_1_url.startswith('/wiki/') and not page_1_name.startswith('This list may not'):
data.append({'page_1_name': page_1_name, 'page_1_url': page_1_url})
df_pages = pd.DataFrame(data)
df_pages['page_id'] = df_pages.index + 1
df_crosslinks = pd.DataFrame(index=df_pages['page_id'], columns=df_pages['page_id']).fillna(0)
for _, row in df_pages.iloc[1:].iterrows():
page_1_id = row['page_id']
page_1_url = row['page_1_url']
page_1_soup = BeautifulSoup(
requests.get('https://en.wikipedia.org' + page_1_url).content,
'html.parser')
links = page_1_soup.find_all('a')
for link in links:
if link.has_attr('href') and link['href'].startswith('/wiki/'):
page_2_url = link['href']
page_2_name = link.text
if page_2_url in df_pages['page_1_url'].values and page_2_url != page_1_url:
page_2_id = df_pages.loc[df_pages['page_1_url'] == page_2_url, 'page_id'].iloc[0]
df_crosslinks.loc[page_1_id, page_2_id] = 1
df_pages = df_pages[['page_id', 'page_1_name']]
print(df_pages)
print(df_crosslinks.iloc[:, -10:])
import networkx as nx
import matplotlib.pyplot as plt
matrix_crosslinks = df_crosslinks.values
G = nx.Graph()
num_nodes = matrix_crosslinks.shape[0]
nodes = range(1, num_nodes+1)
G.add_nodes_from(nodes)
for i in range(num_nodes):
for j in range(num_nodes):
if matrix_crosslinks[i][j] == 1:
G.add_edge(i+1, j+1)
degree = nx.degree_centrality(G)
closeness = nx.closeness_centrality(G)
degree = nx.degree_centrality(G)
closeness = nx.closeness_centrality(G)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name']],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()]}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] ]}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name']],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
plt.figure(figsize = (10, 10))
nx.draw_networkx(G, nx.fruchterman_reingold_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
'Degree Centrality': list(degree.values()), 'Closeness Centrality': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
plt.figure(figsize = (10, 10))
nx.draw_networkx(G, nx.fruchterman_reingold_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name'] for node in G.nodes()],
'Degree': list(degree.values()), 'Closeness': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
data = {'Node': list(G.nodes()), 'Name': [df_pages.loc[node-1, 'page_1_name']],
'Degree': list(degree.values()), 'Closeness': list(closeness.values())}
centrality_df = pd.DataFrame(data)
print(centrality_df)
nx.draw_networkx(G, nx.random_clustered_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
nx.draw_networkx(G, nx.random_layout(G),
plt.show()
plt.show()
nx.draw_networkx(G, nx.spring_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
nx.draw_networkx(G, nx.circular_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
nx.draw_networkx(G, nx.kamada_kawai_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
G.number_of_nodes()
G.number_of_edges()
nx.draw_networkx(G, nx.random_layout(G),
plt.show()
plt.show()
nx.draw_networkx(G, nx.random_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
nx.draw_networkx(G, nx.circular_layout(G),
with_labels = True,
font_size = 8,
node_color = 'aquamarine',
node_size = [degree[node] * 1000 for node in G.nodes()],
edge_color = 'black',
width = 0.5)
plt.axis('off')
plt.show()
print(centrality_df)
list([df_pages.loc['page_1_name']])
ioio = list([df_pages.loc['page_1_name']])
ioio
print(ioio)
